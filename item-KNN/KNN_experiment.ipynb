{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# data science imports\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# metrics imports\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    ret = None\n",
    "    with open(path, 'rb') as f:\n",
    "        ret = pickle.load(f)\n",
    "    return ret\n",
    "\n",
    "def write_pickle(path, data):\n",
    "    with open(path, 'wb') as fw:\n",
    "        pickle.dump(data, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home1/lyt/datasets/yelp_dataset/rates\n"
     ]
    }
   ],
   "source": [
    "homedir = os.getenv('HOME')\n",
    "datapath = os.path.realpath(os.path.join(homedir, 'datasets/yelp_dataset/rates'))\n",
    "print(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = read_pickle(os.path.join(datapath, 'train_data.pickle'))\n",
    "# users = read_pickle(os.path.join(datapath, 'users-complete.pickle'))\n",
    "# items = read_pickle(os.path.join(datapath, 'businesses-complete.pickle'))\n",
    "# test_data = read_pickle(os.path.join(datapath, 'test_with_neg_sample.pickle'))\n",
    "train_data = read_pickle(os.path.join(datapath, 'rate_train'))\n",
    "users = read_pickle(os.path.join(datapath, 'num_to_userid'))\n",
    "items = read_pickle(os.path.join(datapath, 'num_to_businessid'))\n",
    "test_data = read_pickle(os.path.join(datapath, 'test_with_neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business_id': 6024, 'rate': 1.0, 'user_id': 11133, 'timestamp': 1098125200.0}\n",
      "6492\n",
      "{'user_id': 13008, 'pos_business_id': [2217, 1498, 4272, 10321, 1220, 3508, 9361, 2318, 2829, 3073, 11482, 1068, 4811, 10076, 6116, 9515, 10201, 10150, 12376], 'neg_business_id': [11737, 12489, 13866, 7111, 9142, 4656, 11961, 11390, 7289, 6500, 9508, 11147, 8812, 9306, 12889, 11349, 5256, 4459, 3689, 2525, 3959, 7128, 4355, 5813, 5505, 11852, 6591, 3089, 11537, 4905, 6536, 4840, 13801, 3846, 1559, 8957, 13588, 10976, 65, 7695, 13721, 10277, 3236, 7161, 10122, 13057, 5948, 5962, 9833, 5065]}\n",
      "13262\n",
      "13902\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(len(test_data))\n",
    "print(test_data[1])\n",
    "print(len(users))\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6492\n"
     ]
    }
   ],
   "source": [
    "test_users = set(i['user_id'] for i in test_data)\n",
    "print(len(test_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(n_user, n_item, reviews):\n",
    "    \"\"\"\n",
    "    row for item, column for user\n",
    "    \"\"\"\n",
    "    user_ids = [i for i in range(n_user)]\n",
    "    item_ids = [i for i in range(n_item)]\n",
    "    train_mat = np.zeros((n_item, n_user))\n",
    "    for i in reviews:\n",
    "        user = i['user_id']\n",
    "        item = i['business_id']\n",
    "        train_mat[item][user] = 1\n",
    "    return user_ids, item_ids, train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13902, 13262)\n"
     ]
    }
   ],
   "source": [
    "user_ids, item_ids, train_mat = make_matrix(len(users), len(items), train_data)\n",
    "print(train_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 32.846928119659424\n",
      "13902\n",
      "13902\n"
     ]
    }
   ],
   "source": [
    "model = NearestNeighbors(10, algorithm='brute',metric='cosine', p=1)\n",
    "# test hamming\n",
    "model.fit(train_mat)    # the shape of train_mat need to be (n_queries, n_features), thus (n_items, n_users)\n",
    "t0 = time.time()\n",
    "distance, indices = model.kneighbors(train_mat, 11)\n",
    "t1 = time.time()\n",
    "print(\"time cost:\", t1 - t0)\n",
    "print(len(distance))\n",
    "print(len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13902, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.expand_dims(distance[:,1:], -1)\n",
    "b = np.expand_dims(indices[:,1:], -1)\n",
    "predictions = np.concatenate((a, b), axis=2)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_data, train_mat, predictions):\n",
    "    \"\"\"\n",
    "    predictions is calculated above\n",
    "    \"\"\"\n",
    "    precs = []\n",
    "    hrs = []\n",
    "    recalls = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    for i in test_data:\n",
    "        user = i['user_id']\n",
    "        gt_items = i['pos_business_id']\n",
    "#         print(gt_items)\n",
    "        interactions = np.nonzero(train_mat[:,user])\n",
    "#         print(interactions)\n",
    "        for item in gt_items:\n",
    "            try:\n",
    "                assert item not in interactions[0]\n",
    "            except AssertionError:\n",
    "                print(\"user id:\", user)\n",
    "#                 print(gt_items)\n",
    "#                 print(interactions)\n",
    "                print(item)\n",
    "        # predictions[interactions] is the top 10 neighbors of the item\n",
    "        #-----------------------\n",
    "        # step 1: select preds\n",
    "        #-----------------------\n",
    "        unsorted = predictions[interactions].reshape(-1, 2)\n",
    "#         print(\"unsorted:\", unsorted)\n",
    "        #-----------------------\n",
    "        # step 2: sort preds\n",
    "        #-----------------------\n",
    "        sorted_preds = unsorted[np.argsort(unsorted[:, 0])]\n",
    "#         print(\"sorted:\", sorted_preds)\n",
    "        #-----------------------------------------\n",
    "        # step 3: select top 10, but keep unique\n",
    "        #-----------------------------------------\n",
    "        pred_items = []\n",
    "        idx = 0\n",
    "        while(len(pred_items) < 10):\n",
    "            item = int(sorted_preds[idx, 1])\n",
    "            if item not in pred_items:\n",
    "                pred_items.append(item)\n",
    "            idx += 1\n",
    "#         print(\"top10:\", pred_items)\n",
    "        \n",
    "        #-----------------------------\n",
    "        # step 4: Calculate metrics\n",
    "        #-----------------------------\n",
    "        prec = getP(pred_items, gt_items)\n",
    "        hr = getHitRatio(pred_items, gt_items)\n",
    "        recall = getR(pred_items, gt_items)\n",
    "        ndcg = getNDCG(pred_items, gt_items)\n",
    "#         print(\"prec: %.4f, hr: %.4f, recall: %4f, ndcg: %4f\" % (prec, hr, recall, ndcg))\n",
    "        precs.append(prec)\n",
    "        hrs.append(hr)\n",
    "        recalls.append(recall)\n",
    "        ndcgs.append(ndcg)\n",
    "        \n",
    "    mean_prec = np.mean(precs)\n",
    "    mean_hr = np.mean(hrs)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    mean_ndcg = np.mean(ndcgs)\n",
    "    \n",
    "    return mean_prec, mean_hr, mean_recall, mean_ndcg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final: prec@10: 0.0120, hr@10: 0.1198, recall@10: 0.016950, ndcg@10: 0.050127\n"
     ]
    }
   ],
   "source": [
    "prec, hr, recall, ndcg = evaluation(test_data, train_mat, predictions)\n",
    "print(\"final: prec@10: %.4f, hr@10: %.4f, recall@10: %4f, ndcg@10: %4f\" % (prec, hr, recall, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 23.056436777114868\n",
      "cosine final: prec@10: 0.0120, hr@10: 0.1198, recall@10: 0.016950, ndcg@10: 0.050127\n",
      "time cost: 2518.6911492347717\n",
      "hamming final: prec@10: 0.0003, hr@10: 0.0034, recall@10: 0.000369, ndcg@10: 0.002612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 3693.1809375286102\n",
      "jaccard final: prec@10: 0.0121, hr@10: 0.1206, recall@10: 0.017226, ndcg@10: 0.050524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric matching\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric matching\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 3705.852528333664\n",
      "matching final: prec@10: 0.0003, hr@10: 0.0034, recall@10: 0.000369, ndcg@10: 0.002612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric dice\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric dice\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 4836.7737782001495\n",
      "dice final: prec@10: 0.0121, hr@10: 0.1206, recall@10: 0.017226, ndcg@10: 0.050524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/sklearn/metrics/pairwise.py:1735: DataConversionWarning: Data was converted to boolean for metric kulsinski\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "metrics = ['cosine', 'hamming', 'jaccard', 'matching', 'dice', 'kulsinski', 'rogerstanimoto', 'russellrao',\n",
    "          'sokalmichener', 'sokalsneath']\n",
    "for metric in metrics:\n",
    "    model = NearestNeighbors(10, algorithm='brute', metric=metric, p=1)\n",
    "    model.fit(train_mat)    # the shape of train_mat need to be (n_queries, n_features), thus (n_items, n_users)\n",
    "    t0 = time.time()\n",
    "    distance, indices = model.kneighbors(train_mat, 11)\n",
    "    t1 = time.time()\n",
    "    print(\"time cost:\", t1 - t0)\n",
    "    \n",
    "    a = np.expand_dims(distance[:,1:], -1)\n",
    "    b = np.expand_dims(indices[:,1:], -1)\n",
    "    predictions = np.concatenate((a, b), axis=2)\n",
    "    \n",
    "    prec, hr, recall, ndcg = evaluation(test_data, train_mat, predictions)\n",
    "    print(\"%s final: prec@10: %.4f, hr@10: %.4f, recall@10: %4f, ndcg@10: %4f\" % (metric, prec, hr, recall, ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def my_pearson(x, y):\n",
    "    pearson, p_value = pearsonr(x, y)\n",
    "    return -pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/lyt/venv/ONLSTM/lib/python3.5/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "model = NearestNeighbors(10, algorithm='brute', metric=my_pearson, n_jobs=16)\n",
    "model.fit(train_mat)    # the shape of train_mat need to be (n_queries, n_features), thus (n_items, n_users)\n",
    "t0 = time.time()\n",
    "distance, indices = model.kneighbors(train_mat, 11)\n",
    "t1 = time.time()\n",
    "print(\"time cost:\", t1 - t0)\n",
    "\n",
    "a = np.expand_dims(distance[:,1:], -1)\n",
    "b = np.expand_dims(indices[:,1:], -1)\n",
    "predictions = np.concatenate((a, b), axis=2)\n",
    "\n",
    "prec, hr, recall, ndcg = evaluation(test_data, train_mat, predictions)\n",
    "print(\"%s final: prec@10: %.4f, hr@10: %.4f, recall@10: %4f, ndcg@10: %4f\" % (metric, prec, hr, recall, ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13902, 11, 1)\n",
      "(13902, 11, 1)\n"
     ]
    }
   ],
   "source": [
    "m, n = indices.shape\n",
    "a = indices.reshape(m, n, 1)\n",
    "print(a.shape)\n",
    "b = np.expand_dims(indices, -1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36. 36. 36. 36. 36. 36. 36. 36. 36. 36.]\n",
      "[234. 264. 245. 163. 263.  93. 393.  60. 160. 112.]\n",
      "[[ 36. 234.]\n",
      " [ 36. 264.]\n",
      " [ 36. 245.]\n",
      " [ 36. 163.]\n",
      " [ 36. 263.]\n",
      " [ 36.  93.]\n",
      " [ 36. 393.]\n",
      " [ 36.  60.]\n",
      " [ 36. 160.]\n",
      " [ 36. 112.]\n",
      " [ 25. 393.]\n",
      " [ 25. 264.]\n",
      " [ 25. 263.]\n",
      " [ 25. 163.]\n",
      " [ 25. 245.]\n",
      " [ 25. 160.]\n",
      " [ 25. 234.]\n",
      " [ 25.  93.]\n",
      " [ 25. 112.]\n",
      " [ 25.  60.]]\n",
      "\n",
      "[[ 25.  60.]\n",
      " [ 25.  93.]\n",
      " [ 25. 234.]\n",
      " [ 25. 160.]\n",
      " [ 25. 245.]\n",
      " [ 25. 163.]\n",
      " [ 25. 263.]\n",
      " [ 25. 264.]\n",
      " [ 25. 393.]\n",
      " [ 25. 112.]\n",
      " [ 36. 234.]\n",
      " [ 36.  60.]\n",
      " [ 36. 393.]\n",
      " [ 36.  93.]\n",
      " [ 36. 263.]\n",
      " [ 36. 163.]\n",
      " [ 36. 245.]\n",
      " [ 36. 264.]\n",
      " [ 36. 160.]\n",
      " [ 36. 112.]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0,:,0])\n",
    "print(predictions[0,:,1])\n",
    "data = predictions[0:2].reshape(-1, 2)\n",
    "print(data)\n",
    "print()\n",
    "print(data[np.argsort(data[:,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6966, 4858, 601, 11153, 4798, 3553, 3142, 7412, 3659, 7741, 4661, 11908]\n",
      "top10: [2279, 5900, 5124, 5567, 3382, 1918, 3735, 1245, 5914, 1964]\n",
      "0.0\n",
      "0\n",
      "0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "i = test_data[1302]\n",
    "user = i['user_id']\n",
    "gt_items = i['pos_business_id']\n",
    "print(gt_items)\n",
    "interactions = np.nonzero(train_mat[:,user])\n",
    "# predictions[interactions] is the top 10 neighbors of the item\n",
    "unsorted = predictions[interactions].reshape(-1, 2)\n",
    "# print(\"unsorted:\", unsorted)\n",
    "sorted_preds = unsorted[np.argsort(unsorted[:, 0])]\n",
    "# print(\"sorted:\", sorted_preds)\n",
    "pred_items = []\n",
    "idx = 0\n",
    "while(len(pred_items) < 10):\n",
    "    item = int(sorted_preds[idx, 1])\n",
    "    if item not in pred_items:\n",
    "        pred_items.append(item)\n",
    "    idx += 1\n",
    "print(\"top10:\", pred_items)\n",
    "print(getP(pred_items, gt_items))\n",
    "print(getHitRatio(pred_items, gt_items))\n",
    "print(getR(pred_items, gt_items))\n",
    "print(getNDCG(pred_items, gt_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "row = train_mat[:,0]\n",
    "print(col)\n",
    "inter = np.nonzero(col)\n",
    "print(type(inter[0]))\n",
    "for i in inter:\n",
    "    print(col[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ONLSTM",
   "language": "python",
   "name": "onlstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
